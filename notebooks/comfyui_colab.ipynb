{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArtHommage/HommageTools/blob/main/notebooks/comfyui_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaaaaaaaaa"
      },
      "source": [
        "Git clone the repo and install the requirements. (ignore the pip errors about protobuf)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install --upgrade setuptools"
      ],
      "metadata": {
        "id": "XfN5SHhrNFs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bbbbbbbbbb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98540dee-250f-4b33-ca8c-627a8178fc5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive...\n",
            "/\n",
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive\n",
            "/content/drive/MyDrive/ComfyUI\n",
            "-= Updating ComfyUI from main branch =-\n",
            "remote: Enumerating objects: 17, done.\u001b[K\n",
            "remote: Counting objects: 100% (13/13), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2/2), done.\u001b[K\n",
            "remote: Total 17 (delta 11), reused 11 (delta 11), pack-reused 4 (from 1)\u001b[K\n",
            "Unpacking objects: 100% (17/17), 3.97 KiB | 0 bytes/s, done.\n",
            "From https://github.com/comfyanonymous/ComfyUI\n",
            "   0a1f8869..2d17d891  master             -> origin/master\n",
            "   a786ce5e..407a5a65  worksplit-multigpu -> origin/worksplit-multigpu\n",
            "M\t.ci/update_windows/update.py\n",
            "M\t.ci/update_windows/update_comfyui.bat\n",
            "M\t.ci/update_windows/update_comfyui_stable.bat\n",
            "M\t.ci/windows_base_files/README_VERY_IMPORTANT.txt\n",
            "M\t.ci/windows_base_files/run_cpu.bat\n",
            "M\t.ci/windows_base_files/run_nvidia_gpu.bat\n",
            "Already on 'master'\n",
            "Your branch is behind 'origin/master' by 2 commits, and can be fast-forwarded.\n",
            "  (use \"git pull\" to update your local branch)\n",
            "Updating files: 100% (356/356), done.\n",
            "HEAD is now at 2d17d891 Don't error if wan concat image has extra channels.\n",
            "From https://github.com/comfyanonymous/ComfyUI\n",
            " * branch              master     -> FETCH_HEAD\n",
            "Already up to date.\n",
            "-= Install dependencies =-\n",
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu121, https://download.pytorch.org/whl/cu118, https://download.pytorch.org/whl/cu117\n",
            "Collecting xformers!=0.0.18\n",
            "  Downloading https://download.pytorch.org/whl/cu118/xformers-0.0.29.post3-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting comfyui-frontend-package==1.14.6 (from -r requirements.txt (line 1))\n",
            "  Downloading comfyui_frontend_package-1.14.6-py3-none-any.whl.metadata (117 bytes)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (2.6.0+cu124)\n",
            "Collecting torchsde (from -r requirements.txt (line 3))\n",
            "  Downloading torchsde-0.2.6-py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (0.21.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (2.0.2)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (0.8.1)\n",
            "Requirement already satisfied: transformers>=4.28.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 8)) (4.50.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.3 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 9)) (0.21.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 10)) (0.2.0)\n",
            "Requirement already satisfied: safetensors>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 11)) (0.5.3)\n",
            "Requirement already satisfied: aiohttp>=3.11.8 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 12)) (3.11.14)\n",
            "Requirement already satisfied: yarl>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 13)) (1.18.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 14)) (6.0.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 15)) (11.1.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 16)) (1.14.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 17)) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 18)) (5.9.5)\n",
            "Collecting kornia>=0.7.1 (from -r requirements.txt (line 21))\n",
            "  Downloading kornia-0.8.0-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Collecting spandrel (from -r requirements.txt (line 22))\n",
            "  Downloading spandrel-0.4.1-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 23)) (0.13.1)\n",
            "Collecting av (from -r requirements.txt (line 24))\n",
            "  Downloading av-14.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 2)) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 2)) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 2)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 2)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 2)) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->-r requirements.txt (line 2))\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.4.5.8 (from torch->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 2)) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 2)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 2)) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->-r requirements.txt (line 2))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 2)) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 2)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->-r requirements.txt (line 2)) (1.3.0)\n",
            "Collecting trampoline>=0.1.2 (from torchsde->-r requirements.txt (line 3))\n",
            "  Downloading trampoline-0.1.2-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.28.1->-r requirements.txt (line 8)) (0.29.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.28.1->-r requirements.txt (line 8)) (24.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.28.1->-r requirements.txt (line 8)) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers>=4.28.1->-r requirements.txt (line 8)) (2.32.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.11.8->-r requirements.txt (line 12)) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.11.8->-r requirements.txt (line 12)) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.11.8->-r requirements.txt (line 12)) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.11.8->-r requirements.txt (line 12)) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.11.8->-r requirements.txt (line 12)) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.11.8->-r requirements.txt (line 12)) (0.3.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl>=1.18.0->-r requirements.txt (line 13)) (3.10)\n",
            "Collecting kornia_rs>=0.1.0 (from kornia>=0.7.1->-r requirements.txt (line 21))\n",
            "  Downloading kornia_rs-0.1.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile->-r requirements.txt (line 23)) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile->-r requirements.txt (line 23)) (2.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->-r requirements.txt (line 2)) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.28.1->-r requirements.txt (line 8)) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.28.1->-r requirements.txt (line 8)) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.28.1->-r requirements.txt (line 8)) (2025.1.31)\n",
            "Downloading comfyui_frontend_package-1.14.6-py3-none-any.whl (34.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.9/34.9 MB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/cu118/xformers-0.0.29.post3-cp311-cp311-manylinux_2_28_x86_64.whl (13.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.9/13.9 MB\u001b[0m \u001b[31m114.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m118.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m89.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m96.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchsde-0.2.6-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kornia-0.8.0-py2.py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading spandrel-0.4.1-py3-none-any.whl (305 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m305.2/305.2 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading av-14.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (39.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.7/39.7 MB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kornia_rs-0.1.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m73.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trampoline-0.1.2-py3-none-any.whl (5.2 kB)\n",
            "Installing collected packages: trampoline, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, kornia_rs, comfyui-frontend-package, av, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, xformers, torchsde, kornia, spandrel\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed av-14.2.0 comfyui-frontend-package-1.14.6 kornia-0.8.0 kornia_rs-0.1.8 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 spandrel-0.4.1 torchsde-0.2.6 trampoline-0.1.2 xformers-0.0.29.post3\n"
          ]
        }
      ],
      "source": [
        "#@title Environment Setup\n",
        "\n",
        "\n",
        "OPTIONS = {}\n",
        "\n",
        "USE_GOOGLE_DRIVE = True  #@param {type:\"boolean\"}\n",
        "UPDATE_COMFY_UI = True  #@param {type:\"boolean\"}\n",
        "WORKSPACE = 'ComfyUI'\n",
        "OPTIONS['USE_GOOGLE_DRIVE'] = USE_GOOGLE_DRIVE\n",
        "OPTIONS['UPDATE_COMFY_UI'] = UPDATE_COMFY_UI\n",
        "\n",
        "if OPTIONS['USE_GOOGLE_DRIVE']:\n",
        "    !echo \"Mounting Google Drive...\"\n",
        "    %cd /\n",
        "\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    WORKSPACE = \"/content/drive/MyDrive/ComfyUI\"\n",
        "    %cd /content/drive/MyDrive\n",
        "\n",
        "![ ! -d $WORKSPACE ] && echo -= Initial setup ComfyUI =- && git clone https://github.com/comfyanonymous/ComfyUI\n",
        "%cd $WORKSPACE\n",
        "\n",
        "if OPTIONS['UPDATE_COMFY_UI']:\n",
        "  !echo -= Updating ComfyUI from main branch =-\n",
        "  !git fetch origin\n",
        "  !git checkout master\n",
        "  !git reset --hard origin/master\n",
        "  !git pull origin master\n",
        "\n",
        "!echo -= Install dependencies =-\n",
        "!pip install xformers!=0.0.18 -r requirements.txt --extra-index-url https://download.pytorch.org/whl/cu121 --extra-index-url https://download.pytorch.org/whl/cu118 --extra-index-url https://download.pytorch.org/whl/cu117"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cccccccccc"
      },
      "source": [
        "Download some models/checkpoints/vae or custom comfyui nodes (uncomment the commands for the ones you want)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkkkkkkkkkkkkkk"
      },
      "source": [
        "### Run ComfyUI with cloudflared (Recommended Way)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transparent_background piexif ultralytics surrealist blend_modes gguf dynamicprompts pygtrans segment_anything dill boto3 fake_useragent redis fal_client replicate psd-tools onnxruntime albumentations --upgrade\n",
        "!pip install --upgrade transformers==4.49.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BoNqwrsF5xUl",
        "outputId": "2b155906-fbee-4b6c-b3e8-174d3b6680a2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transparent_background\n",
            "  Downloading transparent_background-1.3.3-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting piexif\n",
            "  Downloading piexif-1.1.3-py2.py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.98-py3-none-any.whl.metadata (37 kB)\n",
            "Collecting surrealist\n",
            "  Downloading surrealist-1.1.1-py3-none-any.whl.metadata (34 kB)\n",
            "Collecting blend_modes\n",
            "  Downloading blend_modes-2.2.0-py3-none-any.whl.metadata (7.8 kB)\n",
            "Collecting gguf\n",
            "  Downloading gguf-0.14.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting dynamicprompts\n",
            "  Downloading dynamicprompts-0.31.0-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting pygtrans\n",
            "  Downloading pygtrans-1.6.1-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting segment_anything\n",
            "  Downloading segment_anything-1.0-py3-none-any.whl.metadata (487 bytes)\n",
            "Collecting dill\n",
            "  Downloading dill-0.3.9-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.37.23-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting fake_useragent\n",
            "  Downloading fake_useragent-2.1.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting redis\n",
            "  Downloading redis-5.2.1-py3-none-any.whl.metadata (9.1 kB)\n",
            "Collecting fal_client\n",
            "  Downloading fal_client-0.5.9-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting replicate\n",
            "  Downloading replicate-1.0.4-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.21.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.11/dist-packages (2.0.5)\n",
            "Requirement already satisfied: torch>=1.7.1 in /usr/local/lib/python3.11/dist-packages (from transparent_background) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.11/dist-packages (from transparent_background) (0.21.0+cu124)\n",
            "Requirement already satisfied: opencv-python>=4.6.0.66 in /usr/local/lib/python3.11/dist-packages (from transparent_background) (4.11.0.86)\n",
            "Requirement already satisfied: timm>=0.6.11 in /usr/local/lib/python3.11/dist-packages (from transparent_background) (1.0.15)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.11/dist-packages (from transparent_background) (4.67.1)\n",
            "Requirement already satisfied: kornia>=0.5.4 in /usr/local/lib/python3.11/dist-packages (from transparent_background) (0.8.0)\n",
            "Requirement already satisfied: gdown>=4.5.4 in /usr/local/lib/python3.11/dist-packages (from transparent_background) (5.2.0)\n",
            "Collecting wget>=3.2 (from transparent_background)\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: easydict>=1.10 in /usr/local/lib/python3.11/dist-packages (from transparent_background) (1.13)\n",
            "Requirement already satisfied: pyyaml>=6.0 in /usr/local/lib/python3.11/dist-packages (from transparent_background) (6.0.2)\n",
            "Collecting albucore==0.0.16 (from transparent_background)\n",
            "  Downloading albucore-0.0.16-py3-none-any.whl.metadata (3.1 kB)\n",
            "Collecting flet>=0.23.1 (from transparent_background)\n",
            "  Downloading flet-0.27.6-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.16->transparent_background) (2.0.2)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.16->transparent_background) (4.11.0.86)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.1.0)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.14.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.13.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.11/dist-packages (from surrealist) (1.8.0)\n",
            "Requirement already satisfied: sentencepiece<=0.2.0,>=0.1.98 in /usr/local/lib/python3.11/dist-packages (from gguf) (0.2.0)\n",
            "Requirement already satisfied: jinja2~=3.1 in /usr/local/lib/python3.11/dist-packages (from dynamicprompts) (3.1.6)\n",
            "Requirement already satisfied: pyparsing~=3.0 in /usr/local/lib/python3.11/dist-packages (from dynamicprompts) (3.2.1)\n",
            "Collecting botocore<1.38.0,>=1.37.23 (from boto3)\n",
            "  Downloading botocore-1.37.23-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.12.0,>=0.11.0 (from boto3)\n",
            "  Downloading s3transfer-0.11.4-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: httpx<1,>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from fal_client) (0.28.1)\n",
            "Collecting httpx-sse<0.5,>=0.4.0 (from fal_client)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from replicate) (24.2)\n",
            "Requirement already satisfied: pydantic>1.10.7 in /usr/local/lib/python3.11/dist-packages (from replicate) (2.10.6)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from replicate) (4.12.2)\n",
            "Collecting coloredlogs (from onnxruntime)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (5.29.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (1.13.1)\n",
            "INFO: pip is looking at multiple versions of albumentations to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting albumentations\n",
            "  Downloading albumentations-2.0.5-py3-none-any.whl.metadata (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading albumentations-2.0.4-py3-none-any.whl.metadata (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.5/41.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading albumentations-2.0.3-py3-none-any.whl.metadata (38 kB)\n",
            "  Downloading albumentations-2.0.2-py3-none-any.whl.metadata (38 kB)\n",
            "INFO: pip is still looking at multiple versions of albumentations to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading albumentations-2.0.1-py3-none-any.whl.metadata (38 kB)\n",
            "  Downloading albumentations-2.0.0-py3-none-any.whl.metadata (38 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading albumentations-1.4.24-py3-none-any.whl.metadata (37 kB)\n",
            "  Downloading albumentations-1.4.23-py3-none-any.whl.metadata (36 kB)\n",
            "  Downloading albumentations-1.4.22-py3-none-any.whl.metadata (33 kB)\n",
            "  Downloading albumentations-1.4.21-py3-none-any.whl.metadata (31 kB)\n",
            "  Downloading albumentations-1.4.20-py3-none-any.whl.metadata (32 kB)\n",
            "  Downloading albumentations-1.4.19-py3-none-any.whl.metadata (32 kB)\n",
            "  Downloading albumentations-1.4.18-py3-none-any.whl.metadata (32 kB)\n",
            "Requirement already satisfied: scikit-image>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from albumentations) (0.25.2)\n",
            "  Downloading albumentations-1.4.17-py3-none-any.whl.metadata (38 kB)\n",
            "  Downloading albumentations-1.4.16-py3-none-any.whl.metadata (38 kB)\n",
            "  Downloading albumentations-1.4.15-py3-none-any.whl.metadata (38 kB)\n",
            "Collecting eval-type-backport (from albumentations)\n",
            "  Downloading eval_type_backport-0.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.11/dist-packages (from botocore<1.38.0,>=1.37.23->boto3) (2.8.2)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.11/dist-packages (from botocore<1.38.0,>=1.37.23->boto3) (2.3.0)\n",
            "Requirement already satisfied: oauthlib<4.0.0,>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from flet>=0.23.1->transparent_background) (3.2.2)\n",
            "Collecting repath<0.10.0,>=0.9.0 (from flet>=0.23.1->transparent_background)\n",
            "  Downloading repath-0.9.0-py3-none-any.whl.metadata (899 bytes)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown>=4.5.4->transparent_background) (4.13.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown>=4.5.4->transparent_background) (3.18.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.21.0->fal_client) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.21.0->fal_client) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.21.0->fal_client) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.21.0->fal_client) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.21.0->fal_client) (0.14.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2~=3.1->dynamicprompts) (3.0.2)\n",
            "Requirement already satisfied: kornia_rs>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from kornia>=0.5.4->transparent_background) (0.1.8)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>1.10.7->replicate) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>1.10.7->replicate) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.1)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.21.0->albumentations) (3.4.2)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.21.0->albumentations) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.21.0->albumentations) (2025.3.13)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.21.0->albumentations) (0.4)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from timm>=0.6.11->transparent_background) (0.29.3)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm>=0.6.11->transparent_background) (0.5.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->transparent_background) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->transparent_background) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->transparent_background) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->transparent_background) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->transparent_background) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->transparent_background) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->transparent_background) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->transparent_background) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->transparent_background) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->transparent_background) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->transparent_background) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->transparent_background) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->transparent_background) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->transparent_background) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->transparent_background) (3.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->pygtrans) (1.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.38.0,>=1.37.23->boto3) (1.17.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.21.0->fal_client) (1.3.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown>=4.5.4->transparent_background) (2.6)\n",
            "Downloading transparent_background-1.3.3-py3-none-any.whl (31 kB)\n",
            "Downloading albucore-0.0.16-py3-none-any.whl (9.5 kB)\n",
            "Downloading piexif-1.1.3-py2.py3-none-any.whl (20 kB)\n",
            "Downloading ultralytics-8.3.98-py3-none-any.whl (949 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m950.0/950.0 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading surrealist-1.1.1-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.6/94.6 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading blend_modes-2.2.0-py3-none-any.whl (9.5 kB)\n",
            "Downloading gguf-0.14.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.2/76.2 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dynamicprompts-0.31.0-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.5/53.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pygtrans-1.6.1-py3-none-any.whl (28 kB)\n",
            "Downloading segment_anything-1.0-py3-none-any.whl (36 kB)\n",
            "Downloading dill-0.3.9-py3-none-any.whl (119 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.4/119.4 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading boto3-1.37.23-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.6/139.6 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fake_useragent-2.1.0-py3-none-any.whl (125 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.8/125.8 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading redis-5.2.1-py3-none-any.whl (261 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.5/261.5 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fal_client-0.5.9-py3-none-any.whl (10 kB)\n",
            "Downloading replicate-1.0.4-py3-none-any.whl (48 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.0/48.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.21.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m111.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading albumentations-1.4.15-py3-none-any.whl (200 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.3/200.3 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading botocore-1.37.23-py3-none-any.whl (13.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m121.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading flet-0.27.6-py3-none-any.whl (541 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m541.5/541.5 kB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading s3transfer-0.11.4-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.4/84.4 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\n",
            "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading eval_type_backport-0.2.2-py3-none-any.whl (5.8 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading repath-0.9.0-py3-none-any.whl (4.7 kB)\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9656 sha256=a4eeac18cb98eaa21a56f700bf8635e669faa527e33d2a7ad484bf76e3a270c6\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/b3/0f/a40dbd1c6861731779f62cc4babcb234387e11d697df70ee97\n",
            "Successfully built wget\n",
            "Installing collected packages: wget, segment_anything, surrealist, repath, redis, piexif, jmespath, humanfriendly, httpx-sse, gguf, fake_useragent, eval-type-backport, dill, blend_modes, dynamicprompts, coloredlogs, botocore, albucore, s3transfer, replicate, pygtrans, onnxruntime, flet, fal_client, albumentations, ultralytics-thop, boto3, ultralytics, transparent_background\n",
            "  Attempting uninstall: albucore\n",
            "    Found existing installation: albucore 0.0.23\n",
            "    Uninstalling albucore-0.0.23:\n",
            "      Successfully uninstalled albucore-0.0.23\n",
            "  Attempting uninstall: albumentations\n",
            "    Found existing installation: albumentations 2.0.5\n",
            "    Uninstalling albumentations-2.0.5:\n",
            "      Successfully uninstalled albumentations-2.0.5\n",
            "Successfully installed albucore-0.0.16 albumentations-1.4.15 blend_modes-2.2.0 boto3-1.37.23 botocore-1.37.23 coloredlogs-15.0.1 dill-0.3.9 dynamicprompts-0.31.0 eval-type-backport-0.2.2 fake_useragent-2.1.0 fal_client-0.5.9 flet-0.27.6 gguf-0.14.0 httpx-sse-0.4.0 humanfriendly-10.0 jmespath-1.0.1 onnxruntime-1.21.0 piexif-1.1.3 pygtrans-1.6.1 redis-5.2.1 repath-0.9.0 replicate-1.0.4 s3transfer-0.11.4 segment_anything-1.0 surrealist-1.1.1 transparent_background-1.3.3 ultralytics-8.3.98 ultralytics-thop-2.0.14 wget-3.2\n",
            "Collecting transformers==4.49.0\n",
            "  Downloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.49.0) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.49.0) (0.29.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.49.0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.49.0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.49.0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.49.0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.49.0) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers==4.49.0) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.49.0) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.49.0) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers==4.49.0) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers==4.49.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.49.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.49.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.49.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.49.0) (2025.1.31)\n",
            "Downloading transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m75.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.50.0\n",
            "    Uninstalling transformers-4.50.0:\n",
            "      Successfully uninstalled transformers-4.50.0\n",
            "Successfully installed transformers-4.49.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjjjjjjjjjjjjj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c168c5ee-ea39-41c8-deb1-9d74aaa9fa48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-29 01:19:58--  https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb\n",
            "Resolving github.com (github.com)... 20.27.177.113\n",
            "Connecting to github.com (github.com)|20.27.177.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/cloudflare/cloudflared/releases/download/2025.2.1/cloudflared-linux-amd64.deb [following]\n",
            "--2025-03-29 01:19:58--  https://github.com/cloudflare/cloudflared/releases/download/2025.2.1/cloudflared-linux-amd64.deb\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/106867604/4f284b8f-4452-49e8-a0c0-0e45c3b6e65e?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250329%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250329T011958Z&X-Amz-Expires=300&X-Amz-Signature=f6c2a9a7180d27bbe44ca573e0ba233a79316485562910d94745e725c7b8c2d7&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dcloudflared-linux-amd64.deb&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-03-29 01:19:58--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/106867604/4f284b8f-4452-49e8-a0c0-0e45c3b6e65e?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250329%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250329T011958Z&X-Amz-Expires=300&X-Amz-Signature=f6c2a9a7180d27bbe44ca573e0ba233a79316485562910d94745e725c7b8c2d7&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dcloudflared-linux-amd64.deb&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 18559760 (18M) [application/octet-stream]\n",
            "Saving to: ‘cloudflared-linux-amd64.deb.76’\n",
            "\n",
            "cloudflared-linux-a 100%[===================>]  17.70M  32.4MB/s    in 0.5s    \n",
            "\n",
            "2025-03-29 01:20:00 (32.4 MB/s) - ‘cloudflared-linux-amd64.deb.76’ saved [18559760/18559760]\n",
            "\n",
            "Selecting previously unselected package cloudflared.\n",
            "(Reading database ... 126209 files and directories currently installed.)\n",
            "Preparing to unpack cloudflared-linux-amd64.deb ...\n",
            "Unpacking cloudflared (2025.2.1) ...\n",
            "Setting up cloudflared (2025.2.1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "[START] Security scan\n",
            "[DONE] Security scan\n",
            "## ComfyUI-Manager: installing dependencies done.\n",
            "** ComfyUI startup time: 2025-03-29 01:20:18.997\n",
            "** Platform: Linux\n",
            "** Python version: 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0]\n",
            "** Python executable: /usr/bin/python3\n",
            "** ComfyUI Path: /content/drive/MyDrive/ComfyUI\n",
            "** ComfyUI Base Folder Path: /content/drive/MyDrive/ComfyUI\n",
            "** User directory: /content/drive/MyDrive/ComfyUI/user\n",
            "** ComfyUI-Manager config path: /content/drive/MyDrive/ComfyUI/user/default/ComfyUI-Manager/config.ini\n",
            "** Log path: /content/drive/MyDrive/ComfyUI/user/comfyui.log\n",
            "[ComfyUI-Manager] 'numpy' dependency were fixed\n",
            "\n",
            "Prestartup times for custom nodes:\n",
            "   0.9 seconds: /content/drive/MyDrive/ComfyUI/custom_nodes/rgthree-comfy\n",
            "   1.0 seconds: /content/drive/MyDrive/ComfyUI/custom_nodes/comfyui-easy-use\n",
            "  16.3 seconds: /content/drive/MyDrive/ComfyUI/custom_nodes/comfyui-manager\n",
            "\n",
            "Checkpoint files will always be loaded safely.\n",
            "Total VRAM 15095 MB, total RAM 52217 MB\n",
            "pytorch version: 2.6.0+cu124\n",
            "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
            "    PyTorch 2.6.0+cu118 with CUDA 1108 (you have 2.6.0+cu124)\n",
            "    Python  3.11.11 (you have 3.11.11)\n",
            "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
            "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
            "  Set XFORMERS_MORE_DETAILS=1 for more details\n",
            "xformers version: 0.0.29.post3\n",
            "Set vram state to: NORMAL_VRAM\n",
            "Device: cuda:0 Tesla T4 : cudaMallocAsync\n",
            "Using pytorch attention\n",
            "ComfyUI version: 0.3.27\n",
            "ComfyUI frontend version: 1.14.6\n",
            "[Prompt Server] web root: /usr/local/lib/python3.11/dist-packages/comfyui_frontend_package/static\n",
            "[rvtools \u001b[0;32mINFO\u001b[0m] RvTools v2 Version: 2.3.8\n",
            "/usr/local/lib/python3.11/dist-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.5 (you have 1.4.15). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n",
            "/usr/local/lib/python3.11/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
            "\n",
            "\u001b[92m[rgthree-comfy] Loaded 42 magnificent nodes. 🎉\u001b[00m\n",
            "\n",
            "NumExpr defaulting to 8 threads.\n",
            "2025-03-29 01:26:06.724620: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1743211566.745327    9073 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1743211566.751671    9073 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-29 01:26:06.773544: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "install lark...\n",
            "Package lark installed successfully\n",
            "\u001b[34m[ComfyUI-Easy-Use] server: \u001b[0mv1.2.8 \u001b[92mLoaded\u001b[0m\n",
            "\u001b[34m[ComfyUI-Easy-Use] web root: \u001b[0m/content/drive/MyDrive/ComfyUI/custom_nodes/comfyui-easy-use/web_version/v2 \u001b[92mLoaded\u001b[0m\n",
            "### Loading: ComfyUI-Impact-Pack (V8.8.1)\n",
            "[Impact Pack] Wildcards loading done.\n",
            "\u001b[34mWAS Node Suite: \u001b[0mOpenCV Python FFMPEG support is enabled\u001b[0m\n",
            "\u001b[34mWAS Node Suite \u001b[93mWarning: \u001b[0m`ffmpeg_bin_path` is not set in `/content/drive/MyDrive/ComfyUI/custom_nodes/was-node-suite-comfyui/was_suite_config.json` config file. Will attempt to use system ffmpeg binaries if available.\u001b[0m\n",
            "\u001b[34mWAS Node Suite: \u001b[0mFinished.\u001b[0m \u001b[32mLoaded\u001b[0m \u001b[0m220\u001b[0m \u001b[32mnodes successfully.\u001b[0m\n",
            "\n",
            "\t\u001b[3m\u001b[93m\"The purpose of art is washing the dust of daily life off our souls.\"\u001b[0m\u001b[3m - Pablo Picasso\u001b[0m\n",
            "\n",
            "Adding /content/drive/MyDrive/ComfyUI/custom_nodes to sys.path\n",
            "Could not find efficiency nodes\n",
            "\u001b[36;20m[comfyui_controlnet_aux] | INFO -> Using ckpts path: /content/drive/MyDrive/ComfyUI/custom_nodes/comfyui_controlnet_aux/ckpts\u001b[0m\n",
            "\u001b[36;20m[comfyui_controlnet_aux] | INFO -> Using symlinks: False\u001b[0m\n",
            "\u001b[36;20m[comfyui_controlnet_aux] | INFO -> Using ort providers: ['CUDAExecutionProvider', 'DirectMLExecutionProvider', 'OpenVINOExecutionProvider', 'ROCMExecutionProvider', 'CPUExecutionProvider', 'CoreMLExecutionProvider']\u001b[0m\n",
            "/content/drive/MyDrive/ComfyUI/custom_nodes/comfyui_controlnet_aux/node_wrappers/dwpose.py:26: UserWarning: DWPose: Onnxruntime not found or doesn't come with acceleration providers, switch to OpenCV with CPU device. DWPose might run very slowly\n",
            "  warnings.warn(\"DWPose: Onnxruntime not found or doesn't come with acceleration providers, switch to OpenCV with CPU device. DWPose might run very slowly\")\n",
            "Loaded ControlNetPreprocessors nodes from /content/drive/MyDrive/ComfyUI/custom_nodes/comfyui_controlnet_aux\n",
            "Could not find AdvancedControlNet nodes\n",
            "Could not find AnimateDiff nodes\n",
            "Could not find IPAdapter nodes\n",
            "Could not find VideoHelperSuite nodes\n",
            "Could not load ImpactPack nodes Could not find ImpactPack nodes\n",
            "### Loading: ComfyUI-Impact-Subpack (V1.2.9)\n",
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "[Impact Subpack] ultralytics_bbox: /content/drive/MyDrive/ComfyUI/models/ultralytics/bbox\n",
            "[Impact Subpack] ultralytics_segm: /content/drive/MyDrive/ComfyUI/models/ultralytics/segm\n",
            "------------------------------------------\n",
            "\u001b[34mComfyroll Studio v1.76 : \u001b[92m 175 Nodes Loaded\u001b[0m\n",
            "------------------------------------------\n",
            "** For changes, please see patch notes at https://github.com/Suzie1/ComfyUI_Comfyroll_CustomNodes/blob/main/Patch_Notes.md\n",
            "** For help, please see the wiki at https://github.com/Suzie1/ComfyUI_Comfyroll_CustomNodes/wiki\n",
            "------------------------------------------\n",
            "# 😺dzNodes: LayerStyle -> \u001b[1;33mCannot import name 'guidedFilter' from 'cv2.ximgproc'\n",
            "A few nodes cannot works properly, while most nodes are not affected. Please REINSTALL package 'opencv-contrib-python'.\n",
            "For detail refer to \u001b[4mhttps://github.com/chflame163/ComfyUI_LayerStyle/issues/5\u001b[0m\u001b[m\n",
            "Total VRAM 15095 MB, total RAM 52217 MB\n",
            "pytorch version: 2.6.0+cu124\n",
            "xformers version: 0.0.29.post3\n",
            "Set vram state to: NORMAL_VRAM\n",
            "Device: cuda:0 Tesla T4 : cudaMallocAsync\n",
            "### Loading: ComfyUI-Inspire-Pack (V1.14.1)\n",
            "Loaded Apply Style Model Adjust node - Use this for better control over style vs prompt balance\n",
            "\u001b[92m[tinyterraNodes] \u001b[32mLoaded\u001b[0m\n",
            "### Loading: ComfyUI-Manager (V3.30.9)\n",
            "[ComfyUI-Manager] network_mode: public\n",
            "### ComfyUI Version: v0.3.27-9-g2d17d891 | Released on '2025-03-28'\n",
            "[ComfyUI-Manager] default cache updated: https://raw.githubusercontent.com/ltdrdata/ComfyUI-Manager/main/alter-list.json\n",
            "[ComfyUI-Manager] default cache updated: https://raw.githubusercontent.com/ltdrdata/ComfyUI-Manager/main/github-stats.json\n",
            "[ComfyUI-Manager] default cache updated: https://raw.githubusercontent.com/ltdrdata/ComfyUI-Manager/main/model-list.json\n",
            "(pysssss:WD14Tagger) [DEBUG] Available ORT providers: AzureExecutionProvider, CPUExecutionProvider\n",
            "(pysssss:WD14Tagger) [DEBUG] Using ORT providers: CUDAExecutionProvider, CPUExecutionProvider\n",
            "[ComfyUI-Manager] default cache updated: https://raw.githubusercontent.com/ltdrdata/ComfyUI-Manager/main/custom-node-list.json\n",
            "[ComfyUI-Manager] default cache updated: https://raw.githubusercontent.com/ltdrdata/ComfyUI-Manager/main/extension-node-map.json\n",
            "FETCH ComfyRegistry Data: 5/80\n",
            "FETCH ComfyRegistry Data: 10/80\n",
            "FETCH ComfyRegistry Data: 15/80\n",
            "FETCH ComfyRegistry Data: 20/80\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/ComfyUI/nodes.py\", line 2141, in load_custom_node\n",
            "    module_spec.loader.exec_module(module)\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
            "  File \"/content/drive/MyDrive/ComfyUI/custom_nodes/ComfyUI_InvSR/__init__.py\", line 1, in <module>\n",
            "    from .node import LoadInvSRModels, InvSRSampler\n",
            "  File \"/content/drive/MyDrive/ComfyUI/custom_nodes/ComfyUI_InvSR/node.py\", line 1, in <module>\n",
            "    from .comfyui_invsr_trimmed import get_configs, InvSamplerSR, BaseSampler, Namespace\n",
            "  File \"/content/drive/MyDrive/ComfyUI/custom_nodes/ComfyUI_InvSR/comfyui_invsr_trimmed/__init__.py\", line 1, in <module>\n",
            "    from .inference_invsr import get_configs, Namespace\n",
            "  File \"/content/drive/MyDrive/ComfyUI/custom_nodes/ComfyUI_InvSR/comfyui_invsr_trimmed/inference_invsr.py\", line 7, in <module>\n",
            "    from omegaconf import OmegaConf\n",
            "ModuleNotFoundError: No module named 'omegaconf'\n",
            "\n",
            "Cannot import /content/drive/MyDrive/ComfyUI/custom_nodes/ComfyUI_InvSR module for custom nodes: No module named 'omegaconf'\n",
            "\n",
            "Import times for custom nodes:\n",
            "   0.0 seconds: /content/drive/MyDrive/ComfyUI/custom_nodes/comfyui_controlnet_aux\n",
            "   0.7 seconds: /content/drive/MyDrive/ComfyUI/custom_nodes/websocket_image_save.py\n",
            "   1.2 seconds: /content/drive/MyDrive/ComfyUI/custom_nodes/comfyui-randomsize\n",
            "   1.3 seconds: /content/drive/MyDrive/ComfyUI/custom_nodes/jps-nodes\n",
            "   1.3 seconds: /content/drive/MyDrive/ComfyUI/custom_nodes/masquerade\n",
            "   1.3 seconds: /content/drive/MyDrive/ComfyUI/custom_nodes/ComfyUI-Apply_Style_Model_Adjust\n",
            "   1.3 seconds: /content/drive/MyDrive/ComfyUI/custom_nodes/comfy-image-saver\n",
            "   1.5 seconds: /content/drive/MyDrive/ComfyUI/custom_nodes/comfyui-florence2\n",
            "   1.7 seconds: /content/drive/MyDrive/ComfyUI/custom_nodes/cg-use-everywhere\n",
            "   1.9 seconds: /content/drive/MyDrive/ComfyUI/custom_nodes/comfyui-detail-daemon\n",
            "   2.3 seconds: /content/drive/MyDrive/ComfyUI/custom_nodes/comfyui-ext\n",
            "   3.0 seconds: /content/drive/MyDrive/ComfyUI/custom_nodes/comfyui-impact-subpack\n",
            "   3.1 seconds: /content/drive/MyDrive/ComfyUI/custom_nodes/ComfyUI-GGUF\n",
            "   3.9 seconds: /content/drive/MyDrive/ComfyUI/custom_nodes/comfyui-wd14-tagger\n",
            "   5.3 seconds (IMPORT FAILED): /content/drive/MyDrive/ComfyUI/custom_nodes/ComfyUI_InvSR\n",
            "   5.4 seconds: /content/drive/MyDrive/ComfyUI/custom_nodes/comfyui_essentials\n",
            "   5.9 seconds: /content/drive/MyDrive/ComfyUI/custom_nodes/was-node-suite-comfyui\n",
            "   6.5 seconds: /content/drive/MyDrive/ComfyUI/custom_nodes/ComfyUI_tinyterraNodes\n",
            "   6.7 seconds: /content/drive/MyDrive/ComfyUI/custom_nodes/comfyui-various\n",
            "   7.0 seconds: /content/drive/MyDrive/ComfyUI/custom_nodes/execution-inversion-demo-comfyui\n",
            "   7.3 seconds: /content/drive/MyDrive/ComfyUI/custom_nodes/comfyui-dynamicprompts\n",
            "   7.4 seconds: /content/drive/MyDrive/ComfyUI/custom_nodes/ComfyMath\n",
            "   7.6 seconds: /content/drive/MyDrive/ComfyUI/custom_nodes/comfyui-kjnodes\n",
            "   7.6 seconds: /content/drive/MyDrive/ComfyUI/custom_nodes/comfyui-inspyrenet-rembg\n",
            "   9.3 seconds: /content/drive/MyDrive/ComfyUI/custom_nodes/comfyui_ultimatesdupscale\n",
            "  10.7 seconds: /content/drive/MyDrive/ComfyUI/custom_nodes/comfyui-tensorops\n",
            "  11.7 seconds: /content/drive/MyDrive/ComfyUI/custom_nodes/ComfyUI-Chibi-Nodes\n",
            "  11.9 seconds: /content/drive/MyDrive/ComfyUI/custom_nodes/comfyui-custom-scripts\n",
            "  11.9 seconds: /content/drive/MyDrive/ComfyUI/custom_nodes/comfyui-inspire-pack\n",
            "  13.6 seconds: /content/drive/MyDrive/ComfyUI/custom_nodes/comfyui_nyjy\n",
            "  15.4 seconds: /content/drive/MyDrive/ComfyUI/custom_nodes/comfyui-ppm\n",
            "  15.8 seconds: /content/drive/MyDrive/ComfyUI/custom_nodes/comfyui-impact-pack\n",
            "  20.5 seconds: /content/drive/MyDrive/ComfyUI/custom_nodes/ComfyUI_ExtraModels\n",
            "  25.1 seconds: /content/drive/MyDrive/ComfyUI/custom_nodes/rgthree-comfy\n",
            "  25.1 seconds: /content/drive/MyDrive/ComfyUI/custom_nodes/ComfyUI_Comfyroll_CustomNodes\n",
            "  28.6 seconds: /content/drive/MyDrive/ComfyUI/custom_nodes/HommageTools\n",
            "  67.1 seconds: /content/drive/MyDrive/ComfyUI/custom_nodes/comfyui-art-venture\n",
            "  73.8 seconds: /content/drive/MyDrive/ComfyUI/custom_nodes/comfyui-easy-use\n",
            "  90.6 seconds: /content/drive/MyDrive/ComfyUI/custom_nodes/ComfyUI-RvTools_v2\n",
            "  92.4 seconds: /content/drive/MyDrive/ComfyUI/custom_nodes/comfyui-manager\n",
            " 130.8 seconds: /content/drive/MyDrive/ComfyUI/custom_nodes/comfyui_layerstyle\n",
            "\n",
            "\n",
            "ComfyUI finished loading, trying to launch cloudflared (if it gets stuck here cloudflared is having issues)\n",
            "\n",
            "FETCH ComfyRegistry Data: 25/80\n",
            "This is the URL to access ComfyUI: https://contractors-breeds-lbs-quarters.trycloudflare.com                                 |\n",
            "FETCH ComfyRegistry Data: 30/80\n",
            "FETCH ComfyRegistry Data: 35/80\n",
            "FETCH ComfyRegistry Data: 40/80\n",
            "FETCH ComfyRegistry Data: 45/80\n",
            "FETCH ComfyRegistry Data: 50/80\n",
            "FETCH ComfyRegistry Data: 55/80\n",
            "FETCH ComfyRegistry Data: 60/80\n",
            "FETCH ComfyRegistry Data: 65/80\n",
            "FETCH ComfyRegistry Data: 70/80\n",
            "FETCH ComfyRegistry Data: 75/80\n",
            "FETCH ComfyRegistry Data: 80/80\n",
            "FETCH ComfyRegistry Data [DONE]\n",
            "[ComfyUI-Manager] default cache updated: https://api.comfy.org/nodes\n",
            "nightly_channel: https://raw.githubusercontent.com/ltdrdata/ComfyUI-Manager/main/remote\n",
            "FETCH DATA from: https://raw.githubusercontent.com/ltdrdata/ComfyUI-Manager/main/custom-node-list.json [DONE]\n",
            "[ComfyUI-Manager] All startup tasks have been completed.\n",
            "[Inspire Pack] IPAdapterPlus is not installed.\n",
            "got prompt\n",
            "Loading model from /content/drive/MyDrive/ComfyUI/models/LLM/Florence-2-base\n",
            "Florence2 using sdpa for attention\n",
            "No flash_attn import to remove\n",
            "Florence2LanguageForConditionalGeneration has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
            "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
            "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
            "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
            "Using pytorch attention in VAE\n",
            "Using pytorch attention in VAE\n",
            "VAE load device: cuda:0, offload device: cpu, dtype: torch.float32\n",
            "model weight dtype torch.float8_e4m3fn, manual cast: torch.float16\n",
            "model_type FLUX\n",
            "Requested to load AutoencodingEngine\n",
            "0 models unloaded.\n",
            "loaded partially 128.0 127.99951171875 0\n",
            "CLIP/text encoder model load device: cuda:0, offload device: cpu, current: cpu, dtype: torch.float16\n",
            "Requested to load FluxClipModel_\n",
            "loaded completely 9.5367431640625e+25 9319.23095703125 True\n",
            "</s><s>The image is a black and white photograph of three naked women sitting on the ground in a grassy area. The woman in the center is standing with her arms stretched above her head and her body slightly turned to the side. She has blonde hair and is looking off into the distance with a serious expression on her face. The two women on either side of her are crouching down, with their heads resting on their hands. The background is filled with tall grass and wildflowers. The overall mood of the image is somber and contemplative.</s>\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (109 > 77). Running this sequence through the model will result in indexing errors\n",
            "Requested to load Flux\n",
            "Requested to load ControlNetFlux\n",
            "loaded partially 8045.925265029907 8045.8251953125 0\n",
            "loaded partially 128.0 127.9326171875 0\n",
            "  0% 0/12 [00:00<?, ?it/s]Requested to load AutoencodingEngine\n",
            "0 models unloaded.\n",
            "loaded partially 128.0 127.99951171875 0\n",
            "Requested to load Flux\n",
            "Requested to load ControlNetFlux\n",
            "loaded completely 13046.59663658142 11350.067443847656 True\n",
            "loaded partially 1696.4910457611084 1688.8291015625 0\n",
            "  0% 0/12 [00:31<?, ?it/s]\n",
            "!!! Exception during processing !!! Allocation on device \n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/ComfyUI/execution.py\", line 327, in execute\n",
            "    output_data, output_ui, has_subgraph = get_output_data(obj, input_data_all, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\n",
            "                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/ComfyUI/execution.py\", line 202, in get_output_data\n",
            "    return_values = _map_node_over_list(obj, input_data_all, obj.FUNCTION, allow_interrupt=True, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/ComfyUI/execution.py\", line 174, in _map_node_over_list\n",
            "    process_inputs(input_dict, i)\n",
            "  File \"/content/drive/MyDrive/ComfyUI/execution.py\", line 163, in process_inputs\n",
            "    results.append(getattr(obj, func)(**inputs))\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/ComfyUI/nodes.py\", line 1543, in sample\n",
            "    return common_ksampler(model, seed, steps, cfg, sampler_name, scheduler, positive, negative, latent_image, denoise=denoise)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/ComfyUI/nodes.py\", line 1510, in common_ksampler\n",
            "    samples = comfy.sample.sample(model, noise, steps, cfg, sampler_name, scheduler, positive, negative, latent_image,\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/ComfyUI/custom_nodes/comfyui-impact-pack/modules/impact/sample_error_enhancer.py\", line 22, in informative_sample\n",
            "    raise e\n",
            "  File \"/content/drive/MyDrive/ComfyUI/custom_nodes/comfyui-impact-pack/modules/impact/sample_error_enhancer.py\", line 9, in informative_sample\n",
            "    return original_sample(*args, **kwargs)  # This code helps interpret error messages that occur within exceptions but does not have any impact on other operations.\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/ComfyUI/comfy/sample.py\", line 45, in sample\n",
            "    samples = sampler.sample(noise, positive, negative, cfg=cfg, latent_image=latent_image, start_step=start_step, last_step=last_step, force_full_denoise=force_full_denoise, denoise_mask=noise_mask, sigmas=sigmas, callback=callback, disable_pbar=disable_pbar, seed=seed)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/ComfyUI/comfy/samplers.py\", line 1133, in sample\n",
            "    return sample(self.model, noise, positive, negative, cfg, self.device, sampler, sigmas, self.model_options, latent_image=latent_image, denoise_mask=denoise_mask, callback=callback, disable_pbar=disable_pbar, seed=seed)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/ComfyUI/comfy/samplers.py\", line 1023, in sample\n",
            "    return cfg_guider.sample(noise, latent_image, sampler, sigmas, denoise_mask, callback, disable_pbar, seed)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/ComfyUI/comfy/samplers.py\", line 1008, in sample\n",
            "    output = executor.execute(noise, latent_image, sampler, sigmas, denoise_mask, callback, disable_pbar, seed)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/ComfyUI/comfy/patcher_extension.py\", line 110, in execute\n",
            "    return self.original(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/ComfyUI/comfy/samplers.py\", line 976, in outer_sample\n",
            "    output = self.inner_sample(noise, latent_image, device, sampler, sigmas, denoise_mask, callback, disable_pbar, seed)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/ComfyUI/comfy/samplers.py\", line 959, in inner_sample\n",
            "    samples = executor.execute(self, sigmas, extra_args, callback, noise, latent_image, denoise_mask, disable_pbar)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/ComfyUI/comfy/patcher_extension.py\", line 110, in execute\n",
            "    return self.original(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/ComfyUI/comfy/samplers.py\", line 738, in sample\n",
            "    samples = self.sampler_function(model_k, noise, sigmas, extra_args=extra_args, callback=k_callback, disable=disable_pbar, **self.extra_options)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/ComfyUI/comfy/k_diffusion/sampling.py\", line 741, in sample_dpmpp_2m\n",
            "    denoised = model(x, sigmas[i] * s_in, **extra_args)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/ComfyUI/comfy/samplers.py\", line 390, in __call__\n",
            "    out = self.inner_model(x, sigma, model_options=model_options, seed=seed)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/ComfyUI/comfy/samplers.py\", line 939, in __call__\n",
            "    return self.predict_noise(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/ComfyUI/comfy/samplers.py\", line 942, in predict_noise\n",
            "    return sampling_function(self.inner_model, x, timestep, self.conds.get(\"negative\", None), self.conds.get(\"positive\", None), self.cfg, model_options=model_options, seed=seed)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/ComfyUI/comfy/samplers.py\", line 370, in sampling_function\n",
            "    out = calc_cond_batch(model, conds, x, timestep, model_options)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/ComfyUI/comfy/samplers.py\", line 206, in calc_cond_batch\n",
            "    return executor.execute(model, conds, x_in, timestep, model_options)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/ComfyUI/comfy/patcher_extension.py\", line 110, in execute\n",
            "    return self.original(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/ComfyUI/comfy/samplers.py\", line 314, in _calc_cond_batch\n",
            "    c['control'] = control.get_control(input_x, timestep_, c, len(cond_or_uncond), transformer_options)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/ComfyUI/comfy/controlnet.py\", line 273, in get_control\n",
            "    control = self.control_model(x=x_noisy.to(dtype), hint=self.cond_hint, timesteps=timestep.to(dtype), context=context.to(dtype), **extra)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/ComfyUI/comfy/ldm/flux/controlnet.py\", line 203, in forward\n",
            "    return self.forward_orig(img, img_ids, hint, context, txt_ids, timesteps, y, guidance, control_type=kwargs.get(\"control_type\", []))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/ComfyUI/comfy/ldm/flux/controlnet.py\", line 146, in forward_orig\n",
            "    img, txt = self.double_blocks[i](img=img, txt=txt, vec=vec, pe=pe)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/ComfyUI/comfy/ldm/flux/layers.py\", line 169, in forward\n",
            "    img_qkv = self.img_attn.qkv(img_modulated)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/ComfyUI/comfy/ops.py\", line 71, in forward\n",
            "    return self.forward_comfy_cast_weights(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/ComfyUI/comfy/ops.py\", line 67, in forward_comfy_cast_weights\n",
            "    return torch.nn.functional.linear(input, weight, bias)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "torch.OutOfMemoryError: Allocation on device \n",
            "\n",
            "Got an OOM, unloading all loaded models.\n",
            "Prompt executed in 785.95 seconds\n",
            "got prompt\n",
            "Loading model from /content/drive/MyDrive/ComfyUI/models/LLM/Florence-2-base\n",
            "Florence2 using sdpa for attention\n",
            "No flash_attn import to remove\n",
            "Using pytorch attention in VAE\n",
            "Using pytorch attention in VAE\n",
            "VAE load device: cuda:0, offload device: cpu, dtype: torch.float32\n",
            "model weight dtype torch.float8_e4m3fn, manual cast: torch.float16\n",
            "model_type FLUX\n",
            "Requested to load AutoencodingEngine\n",
            "0 models unloaded.\n",
            "loaded partially 128.0 127.99951171875 0\n",
            "CLIP/text encoder model load device: cuda:0, offload device: cpu, current: cpu, dtype: torch.float16\n",
            "Requested to load FluxClipModel_\n",
            "loaded completely 9.5367431640625e+25 9319.23095703125 True\n",
            "</s><s>The image is a black and white photograph of three naked women sitting on the ground in a grassy area. The woman in the center is standing with her arms stretched above her head and her body slightly turned to the side. She has blonde hair and is looking off to the left with a serious expression on her face. The two women on either side of her are kneeling, with their heads resting on their hands. The background is filled with tall grass and wildflowers. The overall mood of the image is somber and contemplative.</s>\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (108 > 77). Running this sequence through the model will result in indexing errors\n",
            "Requested to load ControlNetFlux\n",
            "Requested to load Flux\n",
            "loaded completely 11371.748171279907 3417.2158203125 True\n",
            "loaded partially 7954.532350967407 7953.200256347656 0\n",
            "  0% 0/12 [00:00<?, ?it/s]Requested to load AutoencodingEngine\n",
            "0 models unloaded.\n",
            "loaded partially 128.0 127.99951171875 0\n",
            "Requested to load ControlNetFlux\n",
            "Requested to load Flux\n",
            "loaded completely 13125.25679283142 3417.2158203125 True\n",
            "loaded partially 9708.04097251892 9707.781311035156 0\n",
            "  0% 0/12 [00:18<?, ?it/s]\n",
            "!!! Exception during processing !!! Allocation on device \n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/ComfyUI/execution.py\", line 327, in execute\n",
            "    output_data, output_ui, has_subgraph = get_output_data(obj, input_data_all, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\n",
            "                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/ComfyUI/execution.py\", line 202, in get_output_data\n",
            "    return_values = _map_node_over_list(obj, input_data_all, obj.FUNCTION, allow_interrupt=True, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/ComfyUI/execution.py\", line 174, in _map_node_over_list\n",
            "    process_inputs(input_dict, i)\n",
            "  File \"/content/drive/MyDrive/ComfyUI/execution.py\", line 163, in process_inputs\n",
            "    results.append(getattr(obj, func)(**inputs))\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/ComfyUI/nodes.py\", line 1543, in sample\n",
            "    return common_ksampler(model, seed, steps, cfg, sampler_name, scheduler, positive, negative, latent_image, denoise=denoise)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/ComfyUI/nodes.py\", line 1510, in common_ksampler\n",
            "    samples = comfy.sample.sample(model, noise, steps, cfg, sampler_name, scheduler, positive, negative, latent_image,\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/ComfyUI/custom_nodes/comfyui-impact-pack/modules/impact/sample_error_enhancer.py\", line 22, in informative_sample\n",
            "    raise e\n",
            "  File \"/content/drive/MyDrive/ComfyUI/custom_nodes/comfyui-impact-pack/modules/impact/sample_error_enhancer.py\", line 9, in informative_sample\n",
            "    return original_sample(*args, **kwargs)  # This code helps interpret error messages that occur within exceptions but does not have any impact on other operations.\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/ComfyUI/comfy/sample.py\", line 45, in sample\n",
            "    samples = sampler.sample(noise, positive, negative, cfg=cfg, latent_image=latent_image, start_step=start_step, last_step=last_step, force_full_denoise=force_full_denoise, denoise_mask=noise_mask, sigmas=sigmas, callback=callback, disable_pbar=disable_pbar, seed=seed)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/ComfyUI/comfy/samplers.py\", line 1133, in sample\n",
            "    return sample(self.model, noise, positive, negative, cfg, self.device, sampler, sigmas, self.model_options, latent_image=latent_image, denoise_mask=denoise_mask, callback=callback, disable_pbar=disable_pbar, seed=seed)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/ComfyUI/comfy/samplers.py\", line 1023, in sample\n",
            "    return cfg_guider.sample(noise, latent_image, sampler, sigmas, denoise_mask, callback, disable_pbar, seed)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/ComfyUI/comfy/samplers.py\", line 1008, in sample\n",
            "    output = executor.execute(noise, latent_image, sampler, sigmas, denoise_mask, callback, disable_pbar, seed)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/ComfyUI/comfy/patcher_extension.py\", line 110, in execute\n",
            "    return self.original(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/ComfyUI/comfy/samplers.py\", line 976, in outer_sample\n",
            "    output = self.inner_sample(noise, latent_image, device, sampler, sigmas, denoise_mask, callback, disable_pbar, seed)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/ComfyUI/comfy/samplers.py\", line 959, in inner_sample\n",
            "    samples = executor.execute(self, sigmas, extra_args, callback, noise, latent_image, denoise_mask, disable_pbar)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/ComfyUI/comfy/patcher_extension.py\", line 110, in execute\n",
            "    return self.original(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/ComfyUI/comfy/samplers.py\", line 738, in sample\n",
            "    samples = self.sampler_function(model_k, noise, sigmas, extra_args=extra_args, callback=k_callback, disable=disable_pbar, **self.extra_options)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/ComfyUI/comfy/k_diffusion/sampling.py\", line 741, in sample_dpmpp_2m\n",
            "    denoised = model(x, sigmas[i] * s_in, **extra_args)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/ComfyUI/comfy/samplers.py\", line 390, in __call__\n",
            "    out = self.inner_model(x, sigma, model_options=model_options, seed=seed)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/ComfyUI/comfy/samplers.py\", line 939, in __call__\n",
            "    return self.predict_noise(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/ComfyUI/comfy/samplers.py\", line 942, in predict_noise\n",
            "    return sampling_function(self.inner_model, x, timestep, self.conds.get(\"negative\", None), self.conds.get(\"positive\", None), self.cfg, model_options=model_options, seed=seed)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/ComfyUI/comfy/samplers.py\", line 370, in sampling_function\n",
            "    out = calc_cond_batch(model, conds, x, timestep, model_options)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/ComfyUI/comfy/samplers.py\", line 206, in calc_cond_batch\n",
            "    return executor.execute(model, conds, x_in, timestep, model_options)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/ComfyUI/comfy/patcher_extension.py\", line 110, in execute\n",
            "    return self.original(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/ComfyUI/comfy/samplers.py\", line 314, in _calc_cond_batch\n",
            "    c['control'] = control.get_control(input_x, timestep_, c, len(cond_or_uncond), transformer_options)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/ComfyUI/comfy/controlnet.py\", line 273, in get_control\n",
            "    control = self.control_model(x=x_noisy.to(dtype), hint=self.cond_hint, timesteps=timestep.to(dtype), context=context.to(dtype), **extra)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/ComfyUI/comfy/ldm/flux/controlnet.py\", line 203, in forward\n",
            "    return self.forward_orig(img, img_ids, hint, context, txt_ids, timesteps, y, guidance, control_type=kwargs.get(\"control_type\", []))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/ComfyUI/comfy/ldm/flux/controlnet.py\", line 146, in forward_orig\n",
            "    img, txt = self.double_blocks[i](img=img, txt=txt, vec=vec, pe=pe)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/ComfyUI/comfy/ldm/flux/layers.py\", line 190, in forward\n",
            "    attn = attention(torch.cat((txt_q, img_q), dim=2),\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/ComfyUI/comfy/ldm/flux/math.py\", line 15, in attention\n",
            "    k = k.to(dtype=pe.dtype).reshape(*k.shape[:-1], -1, 1, 2)\n",
            "        ^^^^^^^^^^^^^^^^^^^^\n",
            "torch.OutOfMemoryError: Allocation on device \n",
            "\n",
            "Got an OOM, unloading all loaded models.\n",
            "Prompt executed in 205.02 seconds\n",
            "got prompt\n",
            "Potential memory leak detected with model FluxClipModel_, doing a full garbage collect, for maximum performance avoid circular references in the model code.\n",
            "\n",
            "HTResolutionDownsampleNode v1.2.0 - Processing\n",
            "Input tensor shape: torch.Size([1, 10417, 7442, 3]), dtype: torch.float32\n",
            "Target long edge: 1280, Interpolation: lanczos\n",
            "Crop to mask: False\n",
            "Input image - Tensor shape: torch.Size([1, 10417, 7442, 3])\n",
            "Input image - Tensor dtype: torch.float32\n",
            "Input image - BHWC format detected\n",
            "Input image - Dimensions: 1x10417x7442x3\n",
            "Image value range: min=0.016, max=0.792\n",
            "Processing device: cuda\n",
            "No mask provided, creating empty mask\n",
            "Processing without cropping\n",
            "Scale factor: 0.123\n",
            "Target dimensions: 914x1280\n",
            "Processing downsample: shape=torch.Size([1, 10417, 7442, 3]) (BHWC)\n",
            "Target size: 914x1280\n",
            "Converted to BCHW: shape=torch.Size([1, 3, 10417, 7442])\n",
            "Converted back to BHWC: shape=torch.Size([1, 1280, 914, 3])\n",
            "Processing mask downsample: shape=torch.Size([1, 10417, 7442])\n",
            "Target size: 914x1280\n",
            "Converted to BCHW: shape=torch.Size([1, 1, 10417, 7442])\n",
            "Converted back to BHW: shape=torch.Size([1, 1280, 914])\n",
            "\n",
            "Output image: shape=torch.Size([1, 1280, 914, 3]) (BHWC)\n",
            "Output mask: shape=torch.Size([1, 1280, 914]) (BHW)\n",
            "Image value range: min=0.037, max=0.783\n",
            "Mask value range: min=0.000, max=0.000\n",
            "CLIP/text encoder model load device: cuda:0, offload device: cpu, current: cpu, dtype: torch.float16\n",
            "/content/drive/MyDrive/ComfyUI/custom_nodes/ComfyUI-GGUF/loader.py:65: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)\n",
            "  torch_tensor = torch.from_numpy(tensor.data) # mmap\n",
            "\n",
            "ggml_sd_loader:\n",
            " 1                             471\n",
            " 12                            304\n",
            " 0                               5\n",
            "model weight dtype torch.float16, manual cast: None\n",
            "model_type FLUX\n",
            "Requested to load CLIPVisionModelProjection\n",
            "loaded completely 13694.9484375 787.7150573730469 True\n",
            "Florence2 using sdpa for attention\n",
            "No flash_attn import to remove\n",
            "!!! Exception during processing !!! Error(s) in loading state_dict for PeftModelForCausalLM:\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.0.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.0.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.0.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.0.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.0.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.0.self_attn.out_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.0.self_attn.out_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.1.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.1.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.1.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.1.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.1.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.1.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.1.self_attn.out_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.1.self_attn.out_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.2.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.2.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.2.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.2.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.2.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.2.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.2.self_attn.out_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.2.self_attn.out_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.3.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.3.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.3.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.3.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.3.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.3.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.3.self_attn.out_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.3.self_attn.out_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.4.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.4.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.4.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.4.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.4.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.4.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.4.self_attn.out_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.4.self_attn.out_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.5.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.5.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.5.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.5.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.5.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.5.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.5.self_attn.out_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.5.self_attn.out_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.0.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.0.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.0.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.0.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.0.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.0.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.0.self_attn.out_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.0.self_attn.out_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.0.encoder_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.0.encoder_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.0.encoder_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.0.encoder_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.0.encoder_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.0.encoder_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.0.encoder_attn.out_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.0.encoder_attn.out_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.1.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.1.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.1.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.1.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.1.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.1.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.1.self_attn.out_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.1.self_attn.out_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.1.encoder_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.1.encoder_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.1.encoder_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.1.encoder_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.1.encoder_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.1.encoder_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.1.encoder_attn.out_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.1.encoder_attn.out_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.2.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.2.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.2.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.2.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.2.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.2.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.2.self_attn.out_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.2.self_attn.out_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.2.encoder_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.2.encoder_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.2.encoder_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.2.encoder_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.2.encoder_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.2.encoder_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.2.encoder_attn.out_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.2.encoder_attn.out_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.3.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.3.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.3.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.3.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.3.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.3.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.3.self_attn.out_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.3.self_attn.out_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.3.encoder_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.3.encoder_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.3.encoder_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.3.encoder_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.3.encoder_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.3.encoder_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.3.encoder_attn.out_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.3.encoder_attn.out_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.4.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.4.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.4.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.4.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.4.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.4.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.4.self_attn.out_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.4.self_attn.out_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.4.encoder_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.4.encoder_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.4.encoder_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.4.encoder_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.4.encoder_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.4.encoder_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.4.encoder_attn.out_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.4.encoder_attn.out_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.5.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.5.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.5.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.5.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.5.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.5.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.5.self_attn.out_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.5.self_attn.out_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.5.encoder_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.5.encoder_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.5.encoder_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.5.encoder_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.5.encoder_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.5.encoder_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.5.encoder_attn.out_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.5.encoder_attn.out_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/ComfyUI/execution.py\", line 327, in execute\n",
            "    output_data, output_ui, has_subgraph = get_output_data(obj, input_data_all, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\n",
            "                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/ComfyUI/execution.py\", line 202, in get_output_data\n",
            "    return_values = _map_node_over_list(obj, input_data_all, obj.FUNCTION, allow_interrupt=True, execution_block_cb=execution_block_cb, pre_execute_cb=pre_execute_cb)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/ComfyUI/execution.py\", line 174, in _map_node_over_list\n",
            "    process_inputs(input_dict, i)\n",
            "  File \"/content/drive/MyDrive/ComfyUI/execution.py\", line 163, in process_inputs\n",
            "    results.append(getattr(obj, func)(**inputs))\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/ComfyUI/custom_nodes/comfyui-florence2/nodes.py\", line 107, in loadmodel\n",
            "    model = PeftModel.from_pretrained(model, adapter_name, trust_remote_code=True)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/peft/peft_model.py\", line 581, in from_pretrained\n",
            "    load_result = model.load_adapter(\n",
            "                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/peft/peft_model.py\", line 1239, in load_adapter\n",
            "    load_result = set_peft_model_state_dict(\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py\", line 451, in set_peft_model_state_dict\n",
            "    load_result = model.load_state_dict(peft_model_state_dict, strict=False)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 2581, in load_state_dict\n",
            "    raise RuntimeError(\n",
            "RuntimeError: Error(s) in loading state_dict for PeftModelForCausalLM:\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.0.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.0.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.0.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.0.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.0.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.0.self_attn.out_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.0.self_attn.out_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.1.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.1.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.1.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.1.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.1.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.1.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.1.self_attn.out_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.1.self_attn.out_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.2.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.2.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.2.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.2.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.2.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.2.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.2.self_attn.out_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.2.self_attn.out_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.3.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.3.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.3.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.3.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.3.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.3.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.3.self_attn.out_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.3.self_attn.out_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.4.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.4.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.4.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.4.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.4.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.4.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.4.self_attn.out_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.4.self_attn.out_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.5.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.5.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.5.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.5.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.5.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.5.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.5.self_attn.out_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.encoder.layers.5.self_attn.out_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.0.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.0.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.0.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.0.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.0.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.0.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.0.self_attn.out_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.0.self_attn.out_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.0.encoder_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.0.encoder_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.0.encoder_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.0.encoder_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.0.encoder_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.0.encoder_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.0.encoder_attn.out_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.0.encoder_attn.out_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.1.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.1.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.1.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.1.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.1.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.1.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.1.self_attn.out_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.1.self_attn.out_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.1.encoder_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.1.encoder_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.1.encoder_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.1.encoder_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.1.encoder_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.1.encoder_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.1.encoder_attn.out_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.1.encoder_attn.out_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.2.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.2.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.2.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.2.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.2.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.2.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.2.self_attn.out_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.2.self_attn.out_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.2.encoder_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.2.encoder_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.2.encoder_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.2.encoder_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.2.encoder_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.2.encoder_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.2.encoder_attn.out_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.2.encoder_attn.out_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.3.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.3.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.3.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.3.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.3.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.3.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.3.self_attn.out_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.3.self_attn.out_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.3.encoder_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.3.encoder_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.3.encoder_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.3.encoder_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.3.encoder_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.3.encoder_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.3.encoder_attn.out_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.3.encoder_attn.out_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.4.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.4.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.4.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.4.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.4.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.4.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.4.self_attn.out_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.4.self_attn.out_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.4.encoder_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.4.encoder_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.4.encoder_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.4.encoder_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.4.encoder_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.4.encoder_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.4.encoder_attn.out_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.4.encoder_attn.out_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.5.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.5.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.5.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.5.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.5.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.5.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.5.self_attn.out_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.5.self_attn.out_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.5.encoder_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.5.encoder_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.5.encoder_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.5.encoder_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.5.encoder_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.5.encoder_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.5.encoder_attn.out_proj.lora_A.default.weight: copying a param with shape torch.Size([8, 768]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
            "\tsize mismatch for base_model.model.language_model.model.decoder.layers.5.encoder_attn.out_proj.lora_B.default.weight: copying a param with shape torch.Size([768, 8]) from checkpoint, the shape in current model is torch.Size([1024, 8]).\n",
            "\n",
            "Prompt executed in 185.49 seconds\n",
            "got prompt\n",
            "\n",
            "HTResolutionDownsampleNode v1.2.0 - Processing\n",
            "Input tensor shape: torch.Size([1, 10417, 7442, 3]), dtype: torch.float32\n",
            "Target long edge: 1280, Interpolation: lanczos\n",
            "Crop to mask: False\n",
            "Input image - Tensor shape: torch.Size([1, 10417, 7442, 3])\n",
            "Input image - Tensor dtype: torch.float32\n",
            "Input image - BHWC format detected\n",
            "Input image - Dimensions: 1x10417x7442x3\n",
            "Image value range: min=0.016, max=0.792\n",
            "Processing device: cuda\n",
            "No mask provided, creating empty mask\n",
            "Processing without cropping\n",
            "Scale factor: 0.123\n",
            "Target dimensions: 914x1280\n",
            "Processing downsample: shape=torch.Size([1, 10417, 7442, 3]) (BHWC)\n",
            "Target size: 914x1280\n",
            "Converted to BCHW: shape=torch.Size([1, 3, 10417, 7442])\n",
            "Converted back to BHWC: shape=torch.Size([1, 1280, 914, 3])\n",
            "Processing mask downsample: shape=torch.Size([1, 10417, 7442])\n",
            "Target size: 914x1280\n",
            "Converted to BCHW: shape=torch.Size([1, 1, 10417, 7442])\n",
            "Converted back to BHW: shape=torch.Size([1, 1280, 914])\n",
            "\n",
            "Output image: shape=torch.Size([1, 1280, 914, 3]) (BHWC)\n",
            "Output mask: shape=torch.Size([1, 1280, 914]) (BHW)\n",
            "Image value range: min=0.037, max=0.783\n",
            "Mask value range: min=0.000, max=0.000\n",
            "Florence2 using sdpa for attention\n",
            "No flash_attn import to remove\n",
            "</s><s>The image is a black and white photograph of three young women in a field. The photograph is taken from a low angle, looking up at the women. The woman in the center is standing with her arms stretched above her head, while the two women on either side of her are kneeling on the ground. All three women are naked, with their bodies facing the camera. The background is filled with tall grass and shrubs, and the overall mood of the image is somber and contemplative.</s>\n",
            "Offloading model...\n",
            "New prompt: (Dramatic full nude naked Photograph:1.3)\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (102 > 77). Running this sequence through the model will result in indexing errors\n",
            "Requested to load FluxClipModel_\n",
            "loaded completely 9.5367431640625e+25 9319.23095703125 True\n",
            "Requested to load Flux\n",
            "loaded completely 12704.993380126953 6610.7598876953125 True\n",
            "100% 8/8 [01:09<00:00,  8.69s/it]\n",
            "Requested to load Flux\n",
            "loaded completely 12830.993330535888 6610.7598876953125 True\n",
            "100% 4/4 [00:28<00:00,  7.17s/it]\n",
            "Requested to load AutoencodingEngine\n",
            "loaded completely 356.88177490234375 319.7467155456543 True\n",
            "Prompt executed in 180.52 seconds\n",
            "got prompt\n",
            "\n",
            "HTResolutionDownsampleNode v1.2.0 - Processing\n",
            "Input tensor shape: torch.Size([1, 10417, 7442, 3]), dtype: torch.float32\n",
            "Target long edge: 1280, Interpolation: lanczos\n",
            "Crop to mask: False\n",
            "Input image - Tensor shape: torch.Size([1, 10417, 7442, 3])\n",
            "Input image - Tensor dtype: torch.float32\n",
            "Input image - BHWC format detected\n",
            "Input image - Dimensions: 1x10417x7442x3\n",
            "Image value range: min=0.016, max=0.792\n",
            "Processing device: cuda\n",
            "No mask provided, creating empty mask\n",
            "Processing without cropping\n",
            "Scale factor: 0.123\n",
            "Target dimensions: 914x1280\n",
            "Processing downsample: shape=torch.Size([1, 10417, 7442, 3]) (BHWC)\n",
            "Target size: 914x1280\n",
            "Converted to BCHW: shape=torch.Size([1, 3, 10417, 7442])\n",
            "Converted back to BHWC: shape=torch.Size([1, 1280, 914, 3])\n",
            "Processing mask downsample: shape=torch.Size([1, 10417, 7442])\n",
            "Target size: 914x1280\n",
            "Converted to BCHW: shape=torch.Size([1, 1, 10417, 7442])\n",
            "Converted back to BHW: shape=torch.Size([1, 1280, 914])\n",
            "\n",
            "Output image: shape=torch.Size([1, 1280, 914, 3]) (BHWC)\n",
            "Output mask: shape=torch.Size([1, 1280, 914]) (BHW)\n",
            "Image value range: min=0.037, max=0.783\n",
            "Mask value range: min=0.000, max=0.000\n",
            "Requested to load CLIPVisionModelProjection\n",
            "loaded completely 8916.841309356689 787.7150573730469 True\n",
            "</s><s>The image is a black and white photograph of three young women in a field. The photograph is taken from a low angle, looking up at the women. The woman in the center is standing with her arms stretched above her head, while the two women on either side of her are kneeling on the ground. All three women are naked, with their bodies facing the camera. The background is filled with tall grass and shrubs, and the overall mood of the image is somber and contemplative.</s>\n",
            "Offloading model...\n",
            "New prompt: (Dramatic  Photograph:1.3)\n",
            "Requested to load Flux\n",
            "loaded completely 12511.246664581298 6610.7598876953125 True\n",
            "100% 8/8 [01:09<00:00,  8.67s/it]\n",
            "Requested to load Flux\n",
            "loaded completely 12511.246614990234 6610.7598876953125 True\n",
            "100% 4/4 [00:28<00:00,  7.11s/it]\n",
            "Prompt executed in 144.85 seconds\n",
            "got prompt\n",
            "\n",
            "HTResolutionDownsampleNode v1.2.0 - Processing\n",
            "Input tensor shape: torch.Size([1, 9678, 7584, 3]), dtype: torch.float32\n",
            "Target long edge: 1280, Interpolation: lanczos\n",
            "Crop to mask: False\n",
            "Input image - Tensor shape: torch.Size([1, 9678, 7584, 3])\n",
            "Input image - Tensor dtype: torch.float32\n",
            "Input image - BHWC format detected\n",
            "Input image - Dimensions: 1x9678x7584x3\n",
            "Image value range: min=0.000, max=1.000\n",
            "Processing device: cuda\n",
            "No mask provided, creating empty mask\n",
            "Processing without cropping\n",
            "Scale factor: 0.132\n",
            "Target dimensions: 1003x1280\n",
            "Processing downsample: shape=torch.Size([1, 9678, 7584, 3]) (BHWC)\n",
            "Target size: 1003x1280\n",
            "Converted to BCHW: shape=torch.Size([1, 3, 9678, 7584])\n",
            "Converted back to BHWC: shape=torch.Size([1, 1280, 1003, 3])\n",
            "Processing mask downsample: shape=torch.Size([1, 9678, 7584])\n",
            "Target size: 1003x1280\n",
            "Converted to BCHW: shape=torch.Size([1, 1, 9678, 7584])\n",
            "Converted back to BHW: shape=torch.Size([1, 1280, 1003])\n",
            "\n",
            "Output image: shape=torch.Size([1, 1280, 1003, 3]) (BHWC)\n",
            "Output mask: shape=torch.Size([1, 1280, 1003]) (BHW)\n",
            "Image value range: min=0.034, max=0.930\n",
            "Mask value range: min=0.000, max=0.000\n",
            "Requested to load CLIPVisionModelProjection\n",
            "loaded completely 8894.847168731689 787.7150573730469 True\n",
            "</s><s>The image is a black and white photograph of two young women standing on a beach. They are both completely naked, with their bodies facing the camera. The woman on the left is standing with her arms around the other woman's waist, while the woman in the middle is standing behind her with her hands on her hips. In the background, there are other people walking on the beach and a dog lying on the sand. The sky is cloudy and the water is calm. The overall mood of the image is peaceful and serene.</s>\n",
            "Offloading model...\n",
            "New prompt: (Dramatic nude naked Photograph:1.3)\n",
            "Requested to load FluxClipModel_\n",
            "loaded completely 9.5367431640625e+25 9319.23095703125 True\n",
            "Requested to load Flux\n",
            "loaded completely 12410.948383331299 6610.7598876953125 True\n",
            "100% 8/8 [01:32<00:00, 11.55s/it]\n",
            "Requested to load Flux\n",
            "loaded completely 12410.948333740234 6610.7598876953125 True\n",
            "100% 4/4 [00:31<00:00,  7.89s/it]\n",
            "Prompt executed in 214.76 seconds\n",
            "got prompt\n",
            "\n",
            "HTResolutionDownsampleNode v1.2.0 - Processing\n",
            "Input tensor shape: torch.Size([1, 5361, 6935, 3]), dtype: torch.float32\n",
            "Target long edge: 1280, Interpolation: lanczos\n",
            "Crop to mask: False\n",
            "Input image - Tensor shape: torch.Size([1, 5361, 6935, 3])\n",
            "Input image - Tensor dtype: torch.float32\n",
            "Input image - BHWC format detected\n",
            "Input image - Dimensions: 1x5361x6935x3\n",
            "Image value range: min=0.004, max=0.992\n",
            "Processing device: cuda\n",
            "No mask provided, creating empty mask\n",
            "Processing without cropping\n",
            "Scale factor: 0.185\n",
            "Target dimensions: 1280x989\n",
            "Processing downsample: shape=torch.Size([1, 5361, 6935, 3]) (BHWC)\n",
            "Target size: 1280x989\n",
            "Converted to BCHW: shape=torch.Size([1, 3, 5361, 6935])\n",
            "Converted back to BHWC: shape=torch.Size([1, 989, 1280, 3])\n",
            "Processing mask downsample: shape=torch.Size([1, 5361, 6935])\n",
            "Target size: 1280x989\n",
            "Converted to BCHW: shape=torch.Size([1, 1, 5361, 6935])\n",
            "Converted back to BHW: shape=torch.Size([1, 989, 1280])\n",
            "\n",
            "Output image: shape=torch.Size([1, 989, 1280, 3]) (BHWC)\n",
            "Output mask: shape=torch.Size([1, 989, 1280]) (BHW)\n",
            "Image value range: min=0.041, max=0.882\n",
            "Mask value range: min=0.000, max=0.000\n",
            "Requested to load CLIPVisionModelProjection\n",
            "loaded completely 9827.540528106689 787.7150573730469 True\n",
            "</s><s>The image is a black and white photograph of a nude woman sitting on a sandy beach. She is facing the camera with her body slightly turned to the side, with her legs spread apart and her arms resting on her knees. Her head is tilted slightly to the left and her eyes are looking directly at the camera. In the background, there are four other people walking on the beach, all of them are blurred. The sky is overcast and the overall mood of the image is somber.</s>\n",
            "Offloading model...\n",
            "New prompt: (Dramatic nude naked Photograph:1.3)\n",
            "Requested to load FluxClipModel_\n",
            "loaded completely 9.5367431640625e+25 9319.23095703125 True\n",
            "Requested to load Flux\n",
            "loaded completely 12429.141820831299 6610.7598876953125 True\n",
            "100% 8/8 [01:29<00:00, 11.14s/it]\n",
            "Requested to load Flux\n",
            "loaded completely 12429.141771240234 6610.7598876953125 True\n",
            "100% 4/4 [00:31<00:00,  7.80s/it]\n",
            "Prompt executed in 182.62 seconds\n",
            "got prompt\n",
            "\n",
            "HTResolutionDownsampleNode v1.2.0 - Processing\n",
            "Input tensor shape: torch.Size([1, 6709, 7906, 3]), dtype: torch.float32\n",
            "Target long edge: 1280, Interpolation: lanczos\n",
            "Crop to mask: False\n",
            "Input image - Tensor shape: torch.Size([1, 6709, 7906, 3])\n",
            "Input image - Tensor dtype: torch.float32\n",
            "Input image - BHWC format detected\n",
            "Input image - Dimensions: 1x6709x7906x3\n",
            "Image value range: min=0.000, max=1.000\n",
            "Processing device: cuda\n",
            "No mask provided, creating empty mask\n",
            "Processing without cropping\n",
            "Scale factor: 0.162\n",
            "Target dimensions: 1280x1086\n",
            "Processing downsample: shape=torch.Size([1, 6709, 7906, 3]) (BHWC)\n",
            "Target size: 1280x1086\n",
            "Converted to BCHW: shape=torch.Size([1, 3, 6709, 7906])\n",
            "Converted back to BHWC: shape=torch.Size([1, 1086, 1280, 3])\n",
            "Processing mask downsample: shape=torch.Size([1, 6709, 7906])\n",
            "Target size: 1280x1086\n",
            "Converted to BCHW: shape=torch.Size([1, 1, 6709, 7906])\n",
            "Converted back to BHW: shape=torch.Size([1, 1086, 1280])\n",
            "\n",
            "Output image: shape=torch.Size([1, 1086, 1280, 3]) (BHWC)\n",
            "Output mask: shape=torch.Size([1, 1086, 1280]) (BHW)\n",
            "Image value range: min=0.031, max=0.996\n",
            "Mask value range: min=0.000, max=0.000\n",
            "Requested to load CLIPVisionModelProjection\n",
            "loaded completely 9663.458496856689 787.7150573730469 True\n",
            "</s><s>The image is a black and white photograph of three people, two women and a young boy, standing in a wooded area. The woman on the left is standing with her back to the camera, facing away from the camera. She is completely naked, with her body slightly turned to the side. The boy is standing in front of her, with his arms around her waist and his head tilted slightly to the left. The two women on the right are standing behind him, with one of them leaning against a tree trunk. The background is blurred, but it appears to be a natural setting with trees and foliage. The overall mood of the image is somber and contemplative.</s>\n",
            "Offloading model...\n",
            "New prompt: (Dramatic full nude naked Photograph:1.3)\n",
            "Requested to load Flux\n",
            "loaded completely 12319.727289581298 6610.7598876953125 True\n",
            " 12% 1/8 [00:11<01:22, 11.82s/it]got prompt\n",
            "100% 8/8 [01:34<00:00, 11.78s/it]\n",
            "Requested to load Flux\n",
            "loaded completely 12319.727239990234 6610.7598876953125 True\n",
            "100% 4/4 [00:33<00:00,  8.49s/it]\n",
            "Prompt executed in 174.98 seconds\n",
            "\n",
            "HTResolutionDownsampleNode v1.2.0 - Processing\n",
            "Input tensor shape: torch.Size([1, 6774, 8144, 3]), dtype: torch.float32\n",
            "Target long edge: 1280, Interpolation: lanczos\n",
            "Crop to mask: False\n",
            "Input image - Tensor shape: torch.Size([1, 6774, 8144, 3])\n",
            "Input image - Tensor dtype: torch.float32\n",
            "Input image - BHWC format detected\n",
            "Input image - Dimensions: 1x6774x8144x3\n",
            "Image value range: min=0.000, max=1.000\n",
            "Processing device: cuda\n",
            "No mask provided, creating empty mask\n",
            "Processing without cropping\n",
            "Scale factor: 0.157\n",
            "Target dimensions: 1280x1065\n",
            "Processing downsample: shape=torch.Size([1, 6774, 8144, 3]) (BHWC)\n",
            "Target size: 1280x1065\n",
            "Converted to BCHW: shape=torch.Size([1, 3, 6774, 8144])\n",
            "Converted back to BHWC: shape=torch.Size([1, 1065, 1280, 3])\n",
            "Processing mask downsample: shape=torch.Size([1, 6774, 8144])\n",
            "Target size: 1280x1065\n",
            "Converted to BCHW: shape=torch.Size([1, 1, 6774, 8144])\n",
            "Converted back to BHW: shape=torch.Size([1, 1065, 1280])\n",
            "\n",
            "Output image: shape=torch.Size([1, 1065, 1280, 3]) (BHWC)\n",
            "Output mask: shape=torch.Size([1, 1065, 1280]) (BHW)\n",
            "Image value range: min=0.045, max=1.005\n",
            "Mask value range: min=0.000, max=0.000\n",
            "Requested to load CLIPVisionModelProjection\n",
            "loaded completely 10680.522949981689 787.7150573730469 True\n",
            "</s><s>The image is a black and white photograph of two young women standing side by side with their arms raised above their heads. They are both naked, with their bodies facing the camera. The woman on the left has long curly hair and is wearing a necklace with a pendant. She has a serious expression on her face and is looking directly at the camera with a slight smile. The background is blurred, but it appears to be a room with a window and a door. The overall mood of the image is sensual and intimate.</s>\n",
            "Offloading model...\n",
            "New prompt: (Dramatic full nude naked Photograph:1.3)\n",
            "Requested to load FluxClipModel_\n",
            "loaded completely 9.5367431640625e+25 9319.23095703125 True\n",
            "Requested to load Flux\n",
            "loaded completely 12338.057445831299 6610.7598876953125 True\n",
            "100% 8/8 [01:42<00:00, 12.75s/it]\n",
            "Requested to load Flux\n",
            "loaded completely 12338.057396240234 6610.7598876953125 True\n",
            "100% 4/4 [00:33<00:00,  8.38s/it]\n",
            "Prompt executed in 197.41 seconds\n",
            "got prompt\n",
            "\n",
            "HTResolutionDownsampleNode v1.2.0 - Processing\n",
            "Input tensor shape: torch.Size([1, 7758, 7256, 3]), dtype: torch.float32\n",
            "Target long edge: 2048, Interpolation: lanczos\n",
            "Crop to mask: False\n",
            "Input image - Tensor shape: torch.Size([1, 7758, 7256, 3])\n",
            "Input image - Tensor dtype: torch.float32\n",
            "Input image - BHWC format detected\n",
            "Input image - Dimensions: 1x7758x7256x3\n",
            "Image value range: min=0.016, max=1.000\n",
            "Processing device: cuda\n",
            "No mask provided, creating empty mask\n",
            "Processing without cropping\n",
            "Scale factor: 0.264\n",
            "Target dimensions: 1915x2048\n",
            "Processing downsample: shape=torch.Size([1, 7758, 7256, 3]) (BHWC)\n",
            "Target size: 1915x2048\n",
            "Converted to BCHW: shape=torch.Size([1, 3, 7758, 7256])\n",
            "Converted back to BHWC: shape=torch.Size([1, 2048, 1915, 3])\n",
            "Processing mask downsample: shape=torch.Size([1, 7758, 7256])\n",
            "Target size: 1915x2048\n",
            "Converted to BCHW: shape=torch.Size([1, 1, 7758, 7256])\n",
            "Converted back to BHW: shape=torch.Size([1, 2048, 1915])\n",
            "\n",
            "Output image: shape=torch.Size([1, 2048, 1915, 3]) (BHWC)\n",
            "Output mask: shape=torch.Size([1, 2048, 1915]) (BHW)\n",
            "Image value range: min=0.035, max=1.005\n",
            "Mask value range: min=0.000, max=0.000\n",
            "Requested to load CLIPVisionModelProjection\n",
            "loaded completely 10464.251465606689 787.7150573730469 True\n",
            "</s><s>The image is a black and white portrait of a young woman standing in front of a dark background. She is completely naked, with her body facing the camera and her arms resting on her hips. Her long blonde hair is styled in loose waves and falls over her shoulders. She has a serious expression on her face and is looking directly at the camera. The overall mood of the image is sensual and alluring.</s>\n",
            "Offloading model...\n",
            "New prompt: (Dramatic nude naked Photograph:1.3)\n",
            "Requested to load Flux\n",
            "loaded completely 10399.946518798828 6610.7598876953125 True\n",
            " 50% 4/8 [02:26<02:26, 36.69s/it]got prompt\n",
            "got prompt\n",
            " 62% 5/8 [03:03<01:49, 36.64s/it]got prompt\n",
            "got prompt\n",
            "got prompt\n",
            "got prompt\n",
            "100% 8/8 [04:53<00:00, 36.64s/it]\n",
            "Requested to load Flux\n",
            "loaded completely 10399.946469207764 6610.7598876953125 True\n",
            "100% 4/4 [02:09<00:00, 32.32s/it]\n",
            "Requested to load AutoencodingEngine\n",
            "0 models unloaded.\n",
            "loaded partially 128.0 127.99951171875 0\n",
            "Prompt executed in 477.04 seconds\n",
            "\n",
            "HTResolutionDownsampleNode v1.2.0 - Processing\n",
            "Input tensor shape: torch.Size([1, 9016, 7243, 3]), dtype: torch.float32\n",
            "Target long edge: 2048, Interpolation: lanczos\n",
            "Crop to mask: False\n",
            "Input image - Tensor shape: torch.Size([1, 9016, 7243, 3])\n",
            "Input image - Tensor dtype: torch.float32\n",
            "Input image - BHWC format detected\n",
            "Input image - Dimensions: 1x9016x7243x3\n",
            "Image value range: min=0.000, max=1.000\n",
            "Processing device: cuda\n",
            "No mask provided, creating empty mask\n",
            "Processing without cropping\n",
            "Scale factor: 0.227\n",
            "Target dimensions: 1645x2048\n",
            "Processing downsample: shape=torch.Size([1, 9016, 7243, 3]) (BHWC)\n",
            "Target size: 1645x2048\n",
            "Converted to BCHW: shape=torch.Size([1, 3, 9016, 7243])\n",
            "Converted back to BHWC: shape=torch.Size([1, 2048, 1645, 3])\n",
            "Processing mask downsample: shape=torch.Size([1, 9016, 7243])\n",
            "Target size: 1645x2048\n",
            "Converted to BCHW: shape=torch.Size([1, 1, 9016, 7243])\n",
            "Converted back to BHW: shape=torch.Size([1, 2048, 1645])\n",
            "\n",
            "Output image: shape=torch.Size([1, 2048, 1645, 3]) (BHWC)\n",
            "Output mask: shape=torch.Size([1, 2048, 1645]) (BHW)\n",
            "Image value range: min=0.001, max=1.008\n",
            "Mask value range: min=0.000, max=0.000\n",
            "Requested to load CLIPVisionModelProjection\n",
            "loaded completely 13533.39423828125 787.7150573730469 True\n",
            "</s><s>The image is a photograph of a naked woman standing on a beach. She is facing away from the camera, with her body slightly turned to the side. Her head is tilted downwards and her eyes are closed, as if she is deep in thought. Her hair is long and flowing, and she is wearing a black bracelet on her left wrist. The background is blurred, but it appears to be a sandy beach with a clear blue sky. The overall mood of the image is somber and contemplative.</s>\n",
            "Offloading model...\n",
            "New prompt: (Dramatic  Photograph:1.3)\n",
            "Requested to load Flux\n",
            "loaded completely 10625.999180908204 6610.7598876953125 True\n",
            "100% 8/8 [04:11<00:00, 31.47s/it]\n",
            "Requested to load Flux\n",
            "loaded completely 10751.99913131714 6610.7598876953125 True\n",
            "100% 4/4 [01:48<00:00, 27.11s/it]\n",
            "0 models unloaded.\n",
            "loaded partially 128.0 127.99951171875 0\n",
            "Prompt executed in 413.62 seconds\n",
            "\n",
            "HTResolutionDownsampleNode v1.2.0 - Processing\n",
            "Input tensor shape: torch.Size([1, 5825, 7362, 3]), dtype: torch.float32\n",
            "Target long edge: 2048, Interpolation: lanczos\n",
            "Crop to mask: False\n",
            "Input image - Tensor shape: torch.Size([1, 5825, 7362, 3])\n",
            "Input image - Tensor dtype: torch.float32\n",
            "Input image - BHWC format detected\n",
            "Input image - Dimensions: 1x5825x7362x3\n",
            "Image value range: min=0.000, max=0.957\n",
            "Processing device: cuda\n",
            "No mask provided, creating empty mask\n",
            "Processing without cropping\n",
            "Scale factor: 0.278\n",
            "Target dimensions: 2048x1620\n",
            "Processing downsample: shape=torch.Size([1, 5825, 7362, 3]) (BHWC)\n",
            "Target size: 2048x1620\n",
            "Converted to BCHW: shape=torch.Size([1, 3, 5825, 7362])\n",
            "Converted back to BHWC: shape=torch.Size([1, 1620, 2048, 3])\n",
            "Processing mask downsample: shape=torch.Size([1, 5825, 7362])\n",
            "Target size: 2048x1620\n",
            "Converted to BCHW: shape=torch.Size([1, 1, 5825, 7362])\n",
            "Converted back to BHW: shape=torch.Size([1, 1620, 2048])\n",
            "\n",
            "Output image: shape=torch.Size([1, 1620, 2048, 3]) (BHWC)\n",
            "Output mask: shape=torch.Size([1, 1620, 2048]) (BHW)\n",
            "Image value range: min=0.025, max=0.908\n",
            "Mask value range: min=0.000, max=0.000\n",
            "Requested to load CLIPVisionModelProjection\n",
            "loaded completely 13534.17548828125 787.7150573730469 True\n",
            "</s><s>The image is a black and white photograph of a naked woman sitting on a rock. The woman is sitting on the left side of the image, with her legs crossed and her arms resting on her knees. She is wearing a black skirt and her breasts are visible. On the right side, there is a young boy wrapped in a white towel, lying on the ground with his head resting on his hands. The boy appears to be sleeping or resting. The background is a rocky wall, and the overall mood of the photograph is somber and contemplative.</s>\n",
            "Offloading model...\n",
            "New prompt: (Dramatic full nude naked Photograph:1.3)\n",
            "Requested to load Flux\n",
            "loaded completely 10669.788430908204 6610.7598876953125 True\n",
            "100% 8/8 [03:49<00:00, 28.67s/it]\n",
            "Requested to load Flux\n",
            "loaded completely 10795.78838131714 6610.7598876953125 True\n",
            "100% 4/4 [01:36<00:00, 24.21s/it]\n",
            "0 models unloaded.\n",
            "loaded partially 128.0 127.99951171875 0\n",
            "Prompt executed in 376.01 seconds\n",
            "got prompt\n",
            "\n",
            "HTResolutionDownsampleNode v1.2.0 - Processing\n",
            "Input tensor shape: torch.Size([1, 7510, 9081, 3]), dtype: torch.float32\n",
            "Target long edge: 1024, Interpolation: lanczos\n",
            "Crop to mask: False\n",
            "Input image - Tensor shape: torch.Size([1, 7510, 9081, 3])\n",
            "Input image - Tensor dtype: torch.float32\n",
            "Input image - BHWC format detected\n",
            "Input image - Dimensions: 1x7510x9081x3\n",
            "Image value range: min=0.020, max=0.996\n",
            "Processing device: cuda\n",
            "No mask provided, creating empty mask\n",
            "Processing without cropping\n",
            "Scale factor: 0.113\n",
            "Target dimensions: 1024x847\n",
            "Processing downsample: shape=torch.Size([1, 7510, 9081, 3]) (BHWC)\n",
            "Target size: 1024x847\n",
            "Converted to BCHW: shape=torch.Size([1, 3, 7510, 9081])\n",
            "Converted back to BHWC: shape=torch.Size([1, 847, 1024, 3])\n",
            "Processing mask downsample: shape=torch.Size([1, 7510, 9081])\n",
            "Target size: 1024x847\n",
            "Converted to BCHW: shape=torch.Size([1, 1, 7510, 9081])\n",
            "Converted back to BHW: shape=torch.Size([1, 847, 1024])\n",
            "\n",
            "Output image: shape=torch.Size([1, 847, 1024, 3]) (BHWC)\n",
            "Output mask: shape=torch.Size([1, 847, 1024]) (BHW)\n",
            "Image value range: min=0.038, max=1.008\n",
            "Mask value range: min=0.000, max=0.000\n",
            "\n",
            "ggml_sd_loader:\n",
            " 1                             471\n",
            " 12                            304\n",
            " 0                               5\n",
            "model weight dtype torch.float16, manual cast: None\n",
            "model_type FLUX\n",
            "Requested to load CLIPVisionModelProjection\n",
            "loaded completely 13571.56611328125 787.7150573730469 True\n",
            "</s><s>The image is a black and white photograph of a young girl sitting on the floor in front of a wooden wall. She is wearing a striped t-shirt and has shoulder-length dark hair with bangs. The girl is looking directly at the camera with a serious expression on her face. In front of her, there is a box of Manischewitz unsalted matzos, a small toy camel, and a small pot. The box is open and the girl is holding it with both hands. The background is blurred, but it appears to be an outdoor setting with a wooden fence and a window.</s>\n",
            "Offloading model...\n",
            "New prompt: (Dramatic nude naked Photograph:1.3)\n",
            "Requested to load FluxClipModel_\n",
            "loaded completely 9.5367431640625e+25 9319.23095703125 True\n",
            "Requested to load Flux\n",
            "loaded completely 12783.851055908202 6610.7598876953125 True\n",
            "100% 8/8 [00:58<00:00,  7.31s/it]\n",
            "Requested to load Flux\n",
            "loaded completely 12909.851006317138 6610.7598876953125 True\n",
            "100% 4/4 [00:23<00:00,  6.00s/it]\n",
            "loaded completely 340.9761657714844 319.7467155456543 True\n",
            "Prompt executed in 244.38 seconds\n",
            "\n",
            "Restarting... [Legacy Mode]\n",
            "\n",
            "\n",
            "Command: ['/usr/bin/python3', 'main.py', '--dont-print-server']\n",
            "[START] Security scan\n",
            "[DONE] Security scan\n",
            "## ComfyUI-Manager: installing dependencies done.\n",
            "** ComfyUI startup time: 2025-03-29 03:51:05.243\n",
            "** Platform: Linux\n",
            "** Python version: 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0]\n",
            "** Python executable: /usr/bin/python3\n",
            "** ComfyUI Path: /content/drive/MyDrive/ComfyUI\n",
            "** ComfyUI Base Folder Path: /content/drive/MyDrive/ComfyUI\n",
            "** User directory: /content/drive/MyDrive/ComfyUI/user\n",
            "** ComfyUI-Manager config path: /content/drive/MyDrive/ComfyUI/user/default/ComfyUI-Manager/config.ini\n",
            "** Log path: /content/drive/MyDrive/ComfyUI/user/comfyui.log\n",
            "\n",
            "Prestartup times for custom nodes:\n",
            "   0.5 seconds: /content/drive/MyDrive/ComfyUI/custom_nodes/comfyui-easy-use\n",
            "   0.5 seconds: /content/drive/MyDrive/ComfyUI/custom_nodes/rgthree-comfy\n",
            "  11.3 seconds: /content/drive/MyDrive/ComfyUI/custom_nodes/comfyui-manager\n",
            "\n",
            "Checkpoint files will always be loaded safely.\n",
            "Total VRAM 15095 MB, total RAM 52217 MB\n",
            "pytorch version: 2.6.0+cu124\n",
            "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
            "    PyTorch 2.6.0+cu118 with CUDA 1108 (you have 2.6.0+cu124)\n",
            "    Python  3.11.11 (you have 3.11.11)\n",
            "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
            "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
            "  Set XFORMERS_MORE_DETAILS=1 for more details\n",
            "xformers version: 0.0.29.post3\n",
            "Set vram state to: NORMAL_VRAM\n",
            "Device: cuda:0 Tesla T4 : cudaMallocAsync\n",
            "Using pytorch attention\n",
            "ComfyUI version: 0.3.27\n",
            "ComfyUI frontend version: 1.14.6\n",
            "[Prompt Server] web root: /usr/local/lib/python3.11/dist-packages/comfyui_frontend_package/static\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/ComfyUI/nodes.py\", line 2141, in load_custom_node\n",
            "    module_spec.loader.exec_module(module)\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
            "  File \"/content/drive/MyDrive/ComfyUI/custom_nodes/HommageTools/__init__.py\", line 20, in <module>\n",
            "    from .nodes.ht_layer_nodes import HTLayerCollectorNode, HTLayerExportNode\n",
            "  File \"/content/drive/MyDrive/ComfyUI/custom_nodes/HommageTools/nodes/ht_layer_nodes.py\", line 15, in <module>\n",
            "    from psd_tools import PSDImage\n",
            "ModuleNotFoundError: No module named 'psd_tools'\n",
            "\n",
            "Cannot import /content/drive/MyDrive/ComfyUI/custom_nodes/HommageTools module for custom nodes: No module named 'psd_tools'\n",
            "[rvtools \u001b[0;32mINFO\u001b[0m] RvTools v2 Version: 2.3.8\n"
          ]
        }
      ],
      "source": [
        "!wget https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb\n",
        "!dpkg -i cloudflared-linux-amd64.deb\n",
        "\n",
        "import subprocess\n",
        "import threading\n",
        "import time\n",
        "import socket\n",
        "import urllib.request\n",
        "\n",
        "def iframe_thread(port):\n",
        "  while True:\n",
        "      time.sleep(0.5)\n",
        "      sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
        "      result = sock.connect_ex(('127.0.0.1', port))\n",
        "      if result == 0:\n",
        "        break\n",
        "      sock.close()\n",
        "  print(\"\\nComfyUI finished loading, trying to launch cloudflared (if it gets stuck here cloudflared is having issues)\\n\")\n",
        "\n",
        "  p = subprocess.Popen([\"cloudflared\", \"tunnel\", \"--url\", \"http://127.0.0.1:{}\".format(port)], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "  for line in p.stderr:\n",
        "    l = line.decode()\n",
        "    if \"trycloudflare.com \" in l:\n",
        "      print(\"This is the URL to access ComfyUI:\", l[l.find(\"http\"):], end='')\n",
        "    #print(l, end='')\n",
        "\n",
        "\n",
        "threading.Thread(target=iframe_thread, daemon=True, args=(8188,)).start()\n",
        "\n",
        "!python main.py --dont-print-server"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}